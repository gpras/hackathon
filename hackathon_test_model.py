# -*- coding: utf-8 -*-
"""Hackathon_test_model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mdL7sDwnKtsLto0XswGucTuqrjobNej9
"""

import numpy as np # linear algebra
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# pip install pyspark
# pip install findspark

import pyspark
import findspark
from pyspark.sql import DataFrame, SparkSession
from typing import List
import pyspark.sql.types as T
import pyspark.sql.functions as F
from pyspark import SparkFiles
spark = SparkSession \
       .builder \
       .appName("SparkByExamples.com") \
       .getOrCreate()
spark

from pyspark.sql.functions import isnull, when, count, col
from pyspark.sql.types import StringType, DoubleType
from pyspark.sql.types import IntegerType
from pyspark.sql.functions import col
from pyspark.ml.feature import StringIndexer
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.feature import PCA as PCAml
from pyspark.ml.regression import LinearRegression
from pyspark.ml import Pipeline
from pyspark.sql.types import StringType, DateType, FloatType

def data_load(df_test):

  # display rows and columns
  row = df_test.count()
  col = len(df_test.columns)
  # printing
  print(f'Dimension of the Dataframe is: {(row,col)}')

  df_test = df_test.replace('NA', None)
  df_test.show()

  return df_test

file1 = spark.read.csv("test.csv",inferSchema=True, header= True)
df_test = data_load(file1)

file2 = spark.read.csv("train.csv",inferSchema=True, header= True)
df_train = data_load(file2)

## Check for datatypes
import pyspark.sql.functions as F

type1 = spark.createDataFrame(
    df_train.dtypes, 'col_name string, dtype string'
).withColumn('dataframe', F.lit('df_train'))

type2 = spark.createDataFrame(
    df_test.dtypes, 'col_name string, dtype string'
).withColumn('dataframe', F.lit('df_test'))

result = type1.join(type2, 'col_name', 'full').select(
    'col_name',
    F.when(type1.dtype != type2.dtype, type1.dtype).alias('df_train'),
    F.when(type1.dtype != type2.dtype, type2.dtype).alias('df_test')
)

result.show(81)

df_test = df_test.withColumn("BsmtFinSF1", df_test["BsmtFinSF1"].cast(IntegerType()))
df_test = df_test.withColumn("BsmtFinSF2",df_test["BsmtFinSF2"].cast(IntegerType()))
df_test = df_test.withColumn("BsmtFullBath",df_test["BsmtFullBath"].cast(IntegerType()))
df_test = df_test.withColumn("BsmtHalfBath",df_test["BsmtHalfBath"].cast(IntegerType()))
df_test = df_test.withColumn("BsmtUnfSF",df_test["BsmtUnfSF"].cast(IntegerType()))
df_test = df_test.withColumn("GarageArea",df_test["GarageArea"].cast(IntegerType()))
df_test = df_test.withColumn("GarageCars",df_test["GarageCars"].cast(IntegerType()))
df_test = df_test.withColumn("TotalBsmtSF",df_test["TotalBsmtSF"].cast(IntegerType()))
# df_test.printSchema()

def stage1_preprocess(df_test):
    df_test.select([count(when(isnull(c), c)).alias(c) for c in df_test.columns]).show()
    df_test_drop = df_test.drop('LotFrontage','Alley','FireplaceQu','PoolQC', 'Fence', 'MiscFeature','Utilities')
    df_test_dropna = df_test_drop.na.drop()
    df_test_dropna.select([count(when(isnull(c), c)).alias(c) for c in df_test_dropna.columns]).show()
    str_cols = [f.name for f in df_test.schema.fields if isinstance(f.dataType, StringType)]
    df_test.select(str_cols).dropDuplicates().show(10)

    intcols = ['MasVnrArea', 'GarageYrBlt']
    for col_name in intcols:
      houseDF = df_test_dropna.withColumn(col_name, col(col_name).cast(IntegerType()))

    return houseDF

houseDF =  stage1_preprocess(df_test)

def stage2_preprocess(houseDF):
    str_cols = [f.name for f in houseDF.schema.fields if isinstance(f.dataType, StringType)]

    ### StringIndexer
    outputCols = []
    for i in str_cols:
        ext = i + '_indexed'
        outputCols.append(ext)

    indexers = StringIndexer(inputCols= str_cols,
                            outputCols=outputCols)
    strindexedDF = indexers.fit(houseDF).transform(houseDF)
    return strindexedDF

strindexedDF =  stage2_preprocess(houseDF)
strindexedDF.show()

strindexedDF.printSchema()

### check the appropriate dataframe before submitting
def stage3_assembler(strindexedDF):

    input_int= []
    int_cols = [f.name for f in strindexedDF.schema.fields if isinstance(f.dataType, IntegerType)]
    for int in int_cols:
      input_int.append(int)
    # print(input_int)

    indexed_str = []
    for str in strindexedDF.columns:
      suffix = "_indexed"
      if str.endswith(suffix):
        indexed_str.append(str)
      # print(indexed_str)

    input_col = input_int + indexed_str
    pipeline_df = strindexedDF.select(input_col)

    return pipeline_df

pipeline_df = stage3_assembler(strindexedDF)
# feature_name = 'SalePrice'
# test_df = pipeline_df.drop(feature_name)

pipeline_df.columns

feature_col = pipeline_df.columns
assembler = VectorAssembler(inputCols=feature_col,outputCol='features')
pca = PCAml(k=2, inputCol='features', outputCol='pcaFeature')
lr = LinearRegression( maxIter=10, regParam=0.3, elasticNetParam=0.8)

# pipeline = Pipeline(stages=[assembler,pca, lr])
# pipeline_model = pipeline.fit(pipeline_df)

"""Saving the model"""

# # use save() method to save the model
# # write().overwrite() is usually used when you want to replace the older model with a new one
# # It might happen that you wish to retrain your model and save it at the same the place
# pipeline_model.write().overwrite().save("lr_model")

"""Loading the model"""

# from google.colab import drive
# drive.mount('/content/gdrive')

# import PipelineModel from pyspark.ml package
from pyspark.ml import PipelineModel

# load the model from the location it is stored
# The loaded model acts as PipelineModel
model_save_name = 'linear_model'
path = F"{model_save_name}"
pipemodel = PipelineModel.load(path)

# ### Predict on the test data
predict = pipemodel.transform(pipeline_df)

# view some of the columns generated
df = predict.select('Id', 'prediction')
df = df.withColumnRenamed('prediction','SalePrice')

# Get the ouput in csv file
df.write.csv("Submission.csv")